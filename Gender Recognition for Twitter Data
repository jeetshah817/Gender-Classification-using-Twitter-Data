Created on Mon Sep  2 12:03:26 2019

@author: Jeet
"""
### IMPORTING PACKAGES
# Basic operations
import numpy as np
import pandas as pd

# Plotting data
import matplotlib.pyplot as plt
import seaborn as sns

# Text manipulation
import re
from nltk.corpus import stopwords

# Converting text data to numeric data
from sklearn.feature_extraction.text import CountVectorizer

# Encoding categories
from sklearn.preprocessing import LabelEncoder

# Cross-validation
from sklearn.model_selection import train_test_split

# Naive Bayes
from sklearn.naive_bayes import MultinomialNB

# Random forest model
from sklearn.ensemble import RandomForestClassifier

# Logistic regression model
from sklearn.linear_model import LogisticRegression

### 
# Importing data
twigen = pd.read_csv("/Users/Jeet/Desktop/gender-classifier-DFE-791531.csv", encoding='latin1')
sorted(twigen)

# Here we consider data points whoes confidence is equal to 1
twigen = twigen[twigen['gender:confidence'] == 1]
twigen = twigen[twigen['gender'] != 'unknown']
twigen.head()

# Cleaning text data
def normalize_text(s):
    # just in case
    s = str(s)
    s = s.lower()
    s = re.sub('\s\W',' ',s) # remove punctuation that is not word-internal (e.g., hyphens, apostrophes)
    s = re.sub('\W\s',' ',s)
    s = re.sub('\s+',' ',s) # make sure we didn't introduce any double spaces
    return s

twigen['text_norm'] = [normalize_text(s) for s in twigen['text']]
twigen['description_norm'] = [normalize_text(s) for s in twigen['description']]

# Removing stopwords from text data
import nltk
nltk.download('stopwords')
stop = set(stopwords.words('english'))
twigen['text_norm'] = twigen['text_norm'].str.lower().str.split()
twigen['text_norm'] = twigen['text_norm'].apply(lambda x : [item for item in x if item not in stop])
twigen['text_norm']

def list(k): # to convert array data to string data
    k = " ".join(k)
    return k

twigen['text_norm_list'] = [list(k) for k in twigen['text_norm']]
twigen['text_norm_list']

# Basic visulization on text data
Male = twigen[twigen['gender'] == 'male']
Female = twigen[twigen['gender'] == 'female']
Brand = twigen[twigen['gender'] == 'brand']
Male_Words = pd.Series(' '.join(Male['text_norm'].astype(str)).lower().split(" ")).value_counts()[:20]
Brand_words = pd.Series(' '.join(Brand['text_norm'].astype(str)).lower().split(" ")).value_counts()[:10]
Female_Words = pd.Series(' '.join(Female['text_norm'].astype(str)).lower().split(" ")).value_counts()[:20]

Female_Words.plot(kind='bar',stacked=True, colormap='Pastel1')
Male_Words.plot(kind='bar',stacked=True, colormap='tab10')
Brand_words.plot(kind='bar',stacked=True, colormap='Pastel2')

sns.countplot(twigen['gender'], label = 'Gender')
sns.barplot (x = 'gender', y = 'fav_number',data = twigen)
sns.barplot (x = 'gender', y = 'retweet_count',data = twigen)

# Converting text data to numeric data
vectorizer = CountVectorizer()
x = vectorizer.fit_transform(twigen['text_norm_list'])
print(x)

# Encoding Dependent variable
encoder = LabelEncoder()
y = encoder.fit_transform(twigen['gender'])
df = pd.DataFrame(y)

# Split into train and test sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)
print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

# Naive bayes model
nb = MultinomialNB()
nb.fit(x_train, y_train)
print("accuracy:", nb.score(x_test, y_test))

# Random forest model
rf = RandomForestClassifier(n_estimators = 100)
rf.fit(x_train,y_train)
print("accuracy: ", rf.score(x_test,y_test))

# Logistic regression model
lr = LogisticRegression()
lr.fit(x_train, y_train)
print("accuracy: ", lr.score(x_test,y_test))

### Hence for the above data we can conclude that Naive Bayes is the best fir to predict the gender of the tweets with 58.27% accuracy.
